# Paper List
Record the papers I have read so far.

## Pretrained Language Models
1. (Transformer) Attention Is All You Need. NeurIPS 2017. [[Paper](https://arxiv.org/pdf/1706.03762.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/plm.md#transformer)]
1. (ELMo) Deep Contextualized Word Representations. NAACL 2018. [[Paper](https://arxiv.org/pdf/1802.05365.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/plm.md#2-elmo)]
1. (GPT) Improving Language Understanding by Generative Pre-Training. OpenAI Tech Report 2018. [[Paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/plm.md#3-gpt)]
1. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL 2019. [[Paper](https://arxiv.org/pdf/1810.04805.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/plm.md#4-bert)]
1. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv 2019. [[Paper](https://arxiv.org/pdf/1907.11692.pdf)]
1. Context-Aware Representations for Knowledge Base Relation Extraction. EMNLP 2017. [[Paper](https://www.aclweb.org/anthology/D17-1188.pdf)]  
1. Language Models as Knowledge Bases?. EMNLP 2019. [[Paper](https://arxiv.org/pdf/1909.01066.pdf)]
1. Learning to Infer Entities, Properties and their Relations from Clinical Conversations. EMNLP 2019. [[Paper](https://arxiv.org/pdf/1908.11536.pdf)]  
1. AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models. EMNLP 2019. [[Paper](https://arxiv.org/pdf/1909.09251.pdf)]
1. Specializing Word Embeddings (for Parsing) by Information Bottleneck. EMNLP 2019. [[Paper](https://arxiv.org/pdf/1910.00163.pdf)]
1. XLNet: Generalized Autoregressive Pretraining for Language Understanding. NeurIPS 2019. [[Paper](https://arxiv.org/pdf/1906.08237.pdf)]
1. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. ACL 2019. [[Paper](https://arxiv.org/pdf/1901.02860.pdf)]
1. ERNIE (Tsinghua): Enhanced Language Representation with Informative Entities. ACL 2019. [[Paper](https://arxiv.org/pdf/1905.07129.pdf)]
1. ERNIE (Baidu): Enhanced Representation through Knowledge Integration. arXiv 2019. [[Paper](https://arxiv.org/pdf/1904.09223.pdf)]
1. SpanBERT: Improving Pre-training by Representing and Predicting Spans. TACL 2020. [[Paper](https://arxiv.org/pdf/1907.10529.pdf)]
1. (MT-DNN) Multi-Task Deep Neural Networks for Natural Language Understanding. ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1441.pdf)]
1. ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding. AAAI 2020. [[Paper](https://arxiv.org/pdf/1907.12412.pdf)]
1. (GPT-2) Language Models are Unsupervised Multitask Learners. OpenAI Tech Report 2019. [[Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)]
1. ELECTRA: Pre-Training Text Encoders as Discriminators Rather Than Generators. ICLR 2020. [[Paper](https://openreview.net/pdf?id=r1xMH1BtvB)]
1. MASS: Masked Sequence to Sequence Pre-training for Language Generation. ICML 2019. [[Paper](https://arxiv.org/pdf/1905.02450.pdf)]
1. (UniLM) Unified Language Model Pre-training for Natural Language Understanding and Generation. NeurIPS 2019. [[Paper](https://arxiv.org/pdf/1905.03197.pdf)]
1. PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. ICML 2020. [[Paper](https://arxiv.org/pdf/1912.08777.pdf)]
1. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. NeurIPS 2019. [[Paper](https://arxiv.org/pdf/1908.02265.pdf)]
1. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. NeurIPS Workshop 2019. [[Paper](https://arxiv.org/pdf/1910.01108.pdf)]  
1. TinyBERT: Distilling BERT for Natural Language Understanding. Findings of EMNLP 2020. [[Paper](https://arxiv.org/pdf/1909.10351.pdf)]  
1. ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations. ICLR 2020. [[Paper](https://arxiv.org/pdf/1909.11942.pdf)]
1. (BERT-PKD) Patient Knowledge Distillation for BERT Model Compression. EMNLP 2019. [[Paper](https://arxiv.org/pdf/1908.09355.pdf)]
1. Lite Transformer with Long-Short Range Attention. ICLR 2020. [[Paper](https://openreview.net/pdf?id=ByeMPlHKPH)]
1. Pay Less Attention with Lightweight and Dynamic Convolutions. ICLR 2019. [[Paper](https://openreview.net/pdf?id=SkVhlh09tX)]
1. BERT-of-Theseus: Compressing BERT by Progressive Module Replacing. EMNLP 2020. [[Paper](https://arxiv.org/pdf/2002.02925.pdf)]
1. MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices. ACL 2020. [[Paper](https://arxiv.org/pdf/2004.02984.pdf)]
1. StructBERT: Incorporating Language Structures into Pretraining for Deep language Understanding. ICLR 2020. [[Paper](https://openreview.net/pdf?id=BJgQ4lSFPH)]
1. (LAMBADA) Do Not Have Enough Data? Deep Learning to the Rescue!. AAAI 2020. [[Paper](https://arxiv.org/pdf/1911.03118.pdf)]
1. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. ACL 2020. [[Paper](https://arxiv.org/pdf/1910.13461.pdf)]
1. Towards Making the Most of BERT in Neural Machine Translation. AAAI 2020. [[Paper](https://arxiv.org/pdf/1908.05672.pdf)]
1. MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning. arXiv 2019. [[paper](https://arxiv.org/pdf/1911.09483.pdf)]
1. K-BERT: Enabling Language Representation with Knowledge Graph. AAAI 2020. [[Paper](https://arxiv.org/pdf/1909.07606.pdf)]
1. Universal Transformers. ICLR 2019. [[Paper](https://openreview.net/pdf?id=HyzdRiR9Y7)]
1. Depth-Adaptive Transformer. ICLR 2020. [[Paper](https://openreview.net/pdf?id=SJg7KhVKPH)]
1. FastBERT: a Self-distilling BERT with Adaptive Inference Time. ACL 2020. [[Paper](https://arxiv.org/pdf/2004.02178.pdf)]
1. DynaBERT: Dynamic BERT with Adaptive Width and Depth. NeurIPS 2020. [[Paper](https://arxiv.org/pdf/2004.04037.pdf)]
1. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. NeurIPS 2020. [[Paper](https://arxiv.org/pdf/2002.10957.pdf)]
1. DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering. ACL 2020. [[Paper](https://arxiv.org/pdf/2005.00697.pdf)]
1. Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing. arXiv 2020. [[Paper](https://arxiv.org/pdf/2006.03236.pdf)]
1. Linformer: Self-Attention with Linear Complexity. arXiv 2020. [[Paper](https://arxiv.org/pdf/2006.04768.pdf)]
1. SqueezeBERT: What can computer vision teach NLP about efficient neural networks?. arXiv 2020. [[Paper](https://arxiv.org/pdf/2006.11316.pdf)]
1. Synthesizer: Rethinking Self-Attention in Transformer Models. arXiv 2020. [[Paper](https://arxiv.org/pdf/2005.00743.pdf)]
1. Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers. ICML 2020. [[Paper](https://arxiv.org/pdf/2002.11794.pdf)]
1. Don't Stop Pretraining: Adapt Language Models to Domains and Tasks. ACL 2020. [[Paper](https://arxiv.org/pdf/2004.10964.pdf)]
1. DeLighT: Very Deep and Light-weight Transformer. arXiv 2020. [[Paper](https://arxiv.org/pdf/2008.00623.pdf)]
1. Generating Long Sequences with Sparse Transformers. arXiv 2019. [[Paper](https://arxiv.org/pdf/1904.10509.pdf)]
1. Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection. arXiv 2019. [[Paper](https://arxiv.org/pdf/1912.11637.pdf)]
1. Longformer: The Long-Document Transformer. arXiv 2020. [[Paper](https://arxiv.org/pdf/2004.05150.pdf)]
1. Big Bird: Transformers for Longer Sequences. NeurIPS 2020. [[Paper](https://arxiv.org/pdf/2007.14062.pdf)]
1. Very Deep Transformers for Neural Machine Translation. arXiv 2020. [[Paper](https://arxiv.org/pdf/2008.07772.pdf)]
1. AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization. arXiv 2020. [[Paper](https://arxiv.org/pdf/2008.11869.pdf)]
1. Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation. arXiv 2020. [[Paper](https://arxiv.org/pdf/2002.10345.pdf)]
1. (Bort) Optimal Subarchitecture Extraction For BERT. arXiv 2020. [[Paper](https://arxiv.org/pdf/2010.10499.pdf)]
1. ConvBERT: Improving BERT with Span-based Dynamic Convolution. NeurIPS 2020. [[Paper](https://arxiv.org/pdf/2008.02496.pdf)]
1. Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation. AAAI 2021. [[Paper](https://www.researchgate.net/publication/348647851_Learning_to_Augment_for_Data-Scarce_Domain_BERT_Knowledge_Distillation)]
1. BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Moverâ€™s Distance. EMNLP 2020. [[Paper](https://arxiv.org/pdf/2010.06133.pdf)]
1. LRC-BERT: Latent-representation Contrastive Knowledge Distillation for Natural Language Understanding. AAAI 2021. [[Paper](https://arxiv.org/pdf/2012.07335.pdf)]
1. EdgeBERT: Optimizing On-Chip Inference for Multi-Task NLP. arXiv 2020. [[Paper](https://arxiv.org/pdf/2011.14203.pdf)]
1. Blockwise Self-Attention for Long Document Understanding. Findings of EMNLP 2020. [[Paper](https://arxiv.org/pdf/1911.02972.pdf)]
1. Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting. AAAI 2021. [[Paper](https://arxiv.org/pdf/2012.07436.pdf)]
1. Prefix-Tuning: Optimizing Continuous Prompts for Generation. arXiv 2021. [[Paper](https://arxiv.org/pdf/2101.00190.pdf)]
1. Sample Efficient Text Summarization Using a Single Pre-Trained Transformer. arXiv 2019. [[Paper](https://arxiv.org/pdf/1905.08836.pdf)]
1. Efficient Adaptation of Pretrained Transformers for Abstractive Summarization. arXiv 2019. [[Paper](https://arxiv.org/pdf/1906.00138.pdf)]
1. Text Summarization with Pretrained Encoders. EMNLP 2019. [[Paper](https://arxiv.org/pdf/1908.08345.pdf)]
1. Pre-trained Summarization Distillation. arXiv 2020. [[Paper](https://arxiv.org/pdf/2010.13002.pdf)]
1. Multi-source Meta Transfer for Low Resource Multiple-Choice Question Answering. ACL 2020. [[Paper](https://www.aclweb.org/anthology/2020.acl-main.654.pdf)]
1. Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining. EMNLP 2020. [[Paper](https://arxiv.org/pdf/2003.13003.pdf)]
1. Pre-training Text Representations as Meta Learning. arXiv 2020. [[Paper](https://arxiv.org/pdf/2004.05568.pdf)]
1. Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains. arXiv 2021. [[Paper](https://arxiv.org/pdf/2012.01266.pdf)]



## Question Answering
1. (DrQA) Reading Wikipedia to Answer Open-Domain Questions. ACL 2017. [[Paper](https://www.aclweb.org/anthology/P17-1171.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#drqa-reading-wikipedia-to-answer-open-domain-questions)]
1. Denoising Distantly Supervised Open-Domain Question Answering. ACL 2018. [[Paper](https://www.aclweb.org/anthology/P18-1161.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#denoising-distantly-supervised-open-domain-question-answering)]
1. Ranking Paragraphs for Improving Answer Recall in Open-Domain Question Answering. EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1053.pdf)]
1. Adaptive Document Retrieval for Deep Question Answering. EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1055.pdf)]
1. RankQA: Neural Question Answering with Answer Re-Ranking. ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1611.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#rankqa-neural-question-answering-with-answer-re-ranking)]
1. (GRAFT-Net) Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text. EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1455.pdf)]
1. Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader. ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1417.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#improving-question-answering-over-incomplete-kbs-with-knowledge-aware-reader)]
1. PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text. EMNLP 2019. [[Paper](https://www.aclweb.org/anthology/D19-1242.pdf)]
1. (MUPPET) Multi-Hop Paragraph Retrieval for Open-Domain Question Answering. ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1222.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#muppet-multi-hop-paragraph-retrieval-for-open-domain-question-answering)]
1. (CopyNet) Incorporating Copying Mechanism in Sequence-to-Sequence Learning. ACL 2016. [[Paper](https://www.aclweb.org/anthology/P16-1154.pdf)]
1. Get To The Point: Summarization with Pointer-Generator Networks. ACL 2017. [[Paper](https://www.aclweb.org/anthology/P17-1099.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#get-to-the-point-summarization-with-pointer-generator-networks)]
1. (QCN) Question Condensing Networks for Answer Selection in Community Question Answering. ACL 2018. [[Paper](https://www.aclweb.org/anthology/P18-1162.pdf)]
1. Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings. EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1452.pdf)]
1. A State-transition Framework to Answer Complex Questions over Knowledge Base. EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1234.pdf)]
1. (SLQA+) Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering. ACL 2018. [[Paper](https://www.aclweb.org/anthology/P18-1158.pdf)]
1. Knowledge-aware Attentive Neural Network for Ranking Question Answer Pairs. SIGIR 2018. [[Paper](https://dl.acm.org/doi/10.1145/3209978.3210081)]
1. Knowledge as A Bridge: Improving Cross-domain Answer Selection with External Knowledge. COLING 2018. [[Paper](https://www.aclweb.org/anthology/C18-1279.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#knowledge-as-a-bridge-improving-cross-domain-answer-selection-with-external-knowledge)]
1. Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge Base Question Answering. AAAI 2019. [[Paper](https://arxiv.org/pdf/1812.02354.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#multi-task-learning-with-multi-view-attention-for-answer-selection-and-knowledge-base-question-answering)]
1. Joint Learning of Answer Selection and Answer Summary Generation in Community Question Answering. AAAI 2020. [[Paper](https://arxiv.org/pdf/1911.09801.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#joint-learning-of-answer-selection-and-answer-summary-generation-in-community-question-answering)]
1. Bridging Hierarchical and Sequential Context Modeling for Question-driven Extractive Answer Summarization. SIGIR 2020. [[Paper](https://dl.acm.org/doi/abs/10.1145/3397271.3401208)]
1. Knowledge Enhanced Latent Relevance Mining for Question Answering. ICASSP 2020. [[Paper](https://ieeexplore.ieee.org/document/9053432)]
1. SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents. AAAI 2017. [[Paper](https://arxiv.org/pdf/1611.04230.pdf)]
1. ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training. Findings of EMNLP 2020. [[Paper](https://arxiv.org/pdf/2001.04063.pdf)]
1. Multi-Fact Correction in Abstractive Text Summarization. EMNLP 2020. [[Paper](https://arxiv.org/pdf/2010.02443.pdf)]
1. HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization. ACL 2019. [[Paper](https://arxiv.org/pdf/1905.06566.pdf)]
1. Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders. ACL 2018. [[Paper](https://www.aclweb.org/anthology/P18-1204.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#learning-to-ask-questions-in-open-domain-conversational-systems-with-typed-decoders)]
1. Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks. EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1424.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/question_generation.md#paragraph-level-neural-question-generation-with-maxout-pointer-and-gated-self-attention-networks-emnlp-2018)]
1. Answer-focused and Position-aware Neural Question Generation. EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1427.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/question_generation.md#answer-focused-and-position-aware-neural-question-generation-emnlp-2018)]
1. Interconnected Question Generation with Coreference Alignment and Conversation Flow Modeling. ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1480.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#interconnected-question-generation-with-coreference-alignment-and-conversation-flow-modeling)]
1. Reinforced Dynamic Reasoning for Conversational Question Generation. ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1203.pdf)]
1. Improving Question Generation With to the Point Context. EMNLP 2019. [[Paper](https://www.aclweb.org/anthology/D19-1317.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/question_generation.md#improving-question-generation-with-to-the-point-context-emnlp-2019)]
1. Let's Ask Again: Refine Network for Automatic Question Generation. EMNLP 2019. [[Paper](https://www.aclweb.org/anthology/D19-1326.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/question_generation.md#lets-ask-again-refine-network-for-automatic-question-generation-emnlp-2019)]
1. Multi-Task Learning with Language Modeling for Question Generation. EMNLP 2019 short. [[Paper](https://www.aclweb.org/anthology/D19-1337.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/question_generation.md#lets-ask-again-refine-network-for-automatic-question-generation-emnlp-2019)]
1. Question-type Driven Question Generation. EMNLP 2019 short. [[Paper](https://www.aclweb.org/anthology/D19-1622.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/question_generation.md#question-type-driven-question-generation-emnlp-2019-short)]
1. A Multi-Agent Communication Framework for Question-Worthy Phrase Extraction and Question Generation. AAAI 2019. [[Paper](https://ojs.aaai.org//index.php/AAAI/article/view/4700)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/question_generation.md#a-multi-agent-communication-framework-for-question-worthy-phrase-extraction-and-question-generation-aaai-2019)]
1. Improving Neural Question Generation using Answer Separation. AAAI 2019. [[Paper](https://arxiv.org/pdf/1809.02393.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/question_generation.md#improving-neural-question-generation-using-answer-separation-aaai2019)]
1. Weak Supervision Enhanced Generative Network for Question Generation. IJCAI 2019. [[Paper](https://arxiv.org/pdf/1907.00607.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/question_generation.md#weak-supervision-enhanced-generative-network-for-question-generation-ijcai-2019)]
1. Learning to Generate Questions by Learning What not to Generate. WWW 2019. [[Paper](https://arxiv.org/pdf/1902.10418.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/question_generation.md#learning-to-generate-questions-by-learning-what-not-to-generate-www-2019)]
1. Learning to Ask More: Semi-Autoregressive Sequential Question Generation under Dual-Graph Interaction. ACL 2020. [[Paper](https://www.aclweb.org/anthology/2020.acl-main.21.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#learning-to-ask-more-semi-autoregressive-sequential-question-generation-under-dual-graph-interaction)]
1. How to Ask Good Questions? Try to Leverage Paraphrases. ACL 2020. [[Paper](https://www.aclweb.org/anthology/2020.acl-main.545.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#how-to-ask-good-questions-try-to-leverage-paraphrases)]
1. PathQG: Neural Question Generation from Facts. EMNLP 2020. [[Paper](https://www.aclweb.org/anthology/2020.emnlp-main.729.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/question_generation.md#pathqg-neural-question-generation-from-facts-emnlp-2020)]
1. Improving Question Generation with Sentence-level Semantic Matching and Answer Position Inferring. AAAI 2020. [[Paper](https://arxiv.org/pdf/1912.00879.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/question_generation.md#improving-question-generation-with-sentence-level-semantic-matching-and-answer-position-inferring-aaai-2020)]
1. Capturing Greater Context for Question Generation. AAAI 2020. [[Paper](https://arxiv.org/pdf/1910.10274.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/question_generation.md#capturing-greater-context-for-question-generation-aaai-2020)]
1. Reinforcement Learning Based Graph-to-Sequence Model for Neural Question Generation. ICLR 2020. [[Paper](https://arxiv.org/pdf/1908.04942.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/question_generation.md#reinforcement-learning-based-graph-to-sequence-model-for-neural-question-generation-iclr-2020)]
1. EQG-RACE: Examination-Type Question Generation. AAAI 2021. [[Paper](https://arxiv.org/pdf/2012.06106.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/question_generation.md#eqg-race-examination-type-question-generation-aaai-2021)]
1. Entity Guided Question Generation with Contextual Structure and Sequence Information Capturing. AAAI 2021. [[Paper](https://www.aaai.org/AAAI21Papers/AAAI-6672.HuangQ.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/question_generation.md#entity-guided-question-generation-with-contextual-structure-and-sequence-information-capturing-aaai-2021)]
1. Attentive User-Engaged Adversarial Neural Network for Community Question Answering. AAAI 2020. [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/6472)]
1. QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering. arXiv 2021. [[Paper](https://arxiv.org/pdf/2104.06378.pdf)]
1. Generating Questions for Knowledge Bases via Incorporating Diversified Contexts and Answer-Aware Loss. EMNLP 2019. [[Paper](https://arxiv.org/pdf/1910.13108.pdf)]


## AutoML
1. AutoML: A Survey of the State-of-the-Art. arXiv 2019. [[Paper](https://arxiv.org/pdf/1908.00709.pdf)]
1. (ENAS) Efficient Neural Architecture Search via Parameter Sharing. ICML 2018. [[Paper](https://arxiv.org/pdf/1802.03268.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/automl.md#enas)]
1. (NASNet) Learning Transferable Architectures for Scalable Image Recognition. CVPR 2018. [[Paper](https://arxiv.org/pdf/1707.07012.pdf)]
1. DARTS: Differentiable Architecture Search. ICLR 2019. [[Paper](https://arxiv.org/pdf/1806.09055.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/automl.md#darts)]
1. FBNet: Hardware-Aware Efficient ConvNet Design. CVPR 2019. [[Paper](https://arxiv.org/pdf/1812.03443.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/automl.md#fbnet)]
1. SNAS: Stochastic Neural Architecture Search. ICLR 2019. [[Paper](https://arxiv.org/pdf/1812.09926.pdf)]
1. (GeNet) Genetic CNN. ICCV 2017. [[Paper](https://arxiv.org/pdf/1703.01513.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/automl.md#genet)]
1. (AmoebaNet) Regularized Evolution for Image Classifier Architecture Search. AAAI 2019. [[Paper](https://arxiv.org/pdf/1802.01548.pdf)]
1. (PNAS) Progressive Neural Architecture Search. ECCV 2018. [[Paper](https://arxiv.org/pdf/1712.00559.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/automl.md#pnas)]
1. ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware. ICLR 2019. [[Paper](https://arxiv.org/pdf/1812.00332.pdf)]
1. (One-Shot) Understanding and Simplifying One-Shot Architecture Search. ICML 2018. [[Paper](http://proceedings.mlr.press/v80/bender18a/bender18a.pdf)]
1. (SPOS) Single Path One-Shot Neural Architecture Search with Uniform Sampling. ECCV 2020. [[Paper](https://arXiv.org/pdf/1904.00420.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/automl.md#spos)]
1. (NAO) Neural Architecture Optimization. NeurIPS 2018. [[Paper](https://arxiv.org/pdf/1808.07233.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/automl.md#nao)]
1. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. ICML 2019. [[Paper](https://arxiv.org/pdf/1905.11946.pdf)]
1. (PC-NAS) Improving One-Shot NAS by Suppressing the Posterior Fading. CVPR 2020. [[Paper](https://openreview.net/pdf?id=HJgJNCEKPr)]
1. FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search. arXiv 2019. [[Paper](https://arxiv.org/pdf/1907.01845.pdf)]
1. (LaNAS) Sample-Efficient Neural Architecture Search by Learning Action Space. arXiv 2019. [[Paper](https://arxiv.org/pdf/1906.06832.pdf)]
1. The Evolved Transformer. ICML 2019. [[Paper](https://arxiv.org/pdf/1901.11117.pdf)]
1. Language Models with Transformers. arXiv 2019. [[Paper](https://arxiv.org/pdf/1904.09408.pdf)]
1. Continual and Multi-Task Architecture Search. ACL 2019. [[Paper](https://arxiv.org/pdf/1906.05226.pdf)]
1. AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture. AAAI 2020. [[Paper](https://arxiv.org/pdf/1911.09251.pdf)]
1. WeNet: Weighted Networks for Recurrent Network Architecture Search. arXiv 2019. [[Paper](https://arxiv.org/pdf/1904.03819.pdf)]
1. Improved Differentiable Architecture Search for Language Modeling and Named Entity Recognition. EMNLP 2019. [[Paper](https://www.aclweb.org/anthology/D19-1367.pdf)]
1. (DNA) Blockwisely Supervised Neural Architecture Search with Knowledge Distillation. CVPR 2020. [[Paper](https://arxiv.org/pdf/1911.13053.pdf)]
1. AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search. IJCAI 2020. [[Paper](https://arxiv.org/pdf/2001.04246.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/nas.md#adabert-task-adaptive-bert-compression-with-differentiable-neural-architecture-search)]
1. AutoML-Zero: Evolving Machine Learning Algorithms From Scratch. ICML 2020. [[Paper](https://arxiv.org/pdf/2003.03384.pdf)]
1. Once-for-All: Train One Network and Specialize it for Efficient Deployment on Diverse Hardware Platforms. ICLR 2020. [[Paper](https://arxiv.org/pdf/1908.09791.pdf)]
1. Are Labels Necessary for Neural Architecture Search?. ECCV 2020. [[Paper](https://arxiv.org/pdf/2003.12056.pdf)]
1. BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models. ECCV 2020. [[Paper](https://arxiv.org/pdf/2003.11142.pdf)]
1. Neural Architecture Search for Lightweight Non-Local Networks. CVPR 2020. [[Paper](https://arxiv.org/pdf/2004.01961.pdf)]
1. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing. ACL 2020. [[Paper](https://arxiv.org/pdf/2005.14187.pdf)]
1. FBNetV2: Differentiable Neural Architecture Search for Spatial and Channel Dimensions. CVPR 2020. [[Paper](https://arxiv.org/pdf/2004.05565.pdf)]
1. FBNetV3: Joint Architecture-Recipe Search using Neural Acquisition Function. arXiv 2020. [[Paper](https://arxiv.org/pdf/2006.02049.pdf)]
1. Evolving Normalization-Activation Layers. arXiv 2020. [[Paper](https://arxiv.org/pdf/2004.02967.pdf)]
1. Computation Reallocation for Object Detection. ICLR 2020. [[Paper](https://arxiv.org/pdf/1912.11234.pdf)]
1. Learning Dynamic Routing for Semantic Segmentation. CVPR 2020. [[Paper](https://arxiv.org/pdf/2003.10401.pdf)]
1. Slimmable Neural Networks. ICLR 2019. [[Paper](https://openreview.net/pdf?id=H1gMCsAqY7)]
1. Universally Slimmable Networks and Improved Training Techniques. ICCV 2019. [[Paper](https://arxiv.org/pdf/1903.05134.pdf)]
1. AutoSlim: Towards One-Shot Architecture Search for Channel Numbers. arXiv 2019. [[Paper](https://arxiv.org/pdf/1903.11728.pdf)]
1. Resolution Adaptive Networks for Efficient Inference. CVPR 2020. [[Paper](https://arxiv.org/pdf/2003.07326.pdf)]
1. AutoTrans: Automating Transformer Design via Reinforced Architecture. arXiv 2020. [[Paper](https://arxiv.org/pdf/2009.02070.pdf)]
1. Tiny Transfer Learning: Towards Memory-Efficient On-Device Learning. arXiv 2020. [[Paper](https://arxiv.org/pdf/2007.11622.pdf)]
1. Task-Agnostic and Adaptive-Size BERT Compression. arXiv 2020. [[Paper](https://openreview.net/pdf?id=wZ4yWvQ_g2y)]
1. Interactive Feature Generation via Learning Adjacency Tensor of Feature Graph. arXiv 2020. [[Paper](https://arxiv.org/pdf/2007.14573.pdf)]
1. Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search. ICLR 2021. [[Paper](https://openreview.net/pdf?id=5jzlpHvvRk)]
1. Hierarchically Structured Meta-learning. ICML 2019. [[Paper](https://arxiv.org/pdf/1905.05301.pdf)]
1. Automated Relational Meta-learning. ICLR 2020. [[Paper](https://arxiv.org/pdf/2001.00745.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/meta_learning.md#automated-relational-meta-learning)]
1. Online Structured Meta-learning. NeurIPS 2020. [[Paper](https://arxiv.org/pdf/2010.11545.pdf)]
