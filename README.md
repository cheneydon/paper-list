# Paper List
Record the papers I have read so far.

## Question Answering
1. (DrQA) Reading Wikipedia to Answer Open-Domain Questions, ACL 2017. [[Paper](https://www.aclweb.org/anthology/P17-1171.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/qa.md#drqa-reading-wikipedia-to-answer-open-domain-questions)]
2. Denoising Distantly Supervised Open-Domain Question Answering, ACL 2018. [[Paper](https://www.aclweb.org/anthology/P18-1161.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/qa.md#denoising-distantly-supervised-open-domain-question-answering)]
3. Ranking Paragraphs for Improving Answer Recall in Open-Domain Question Answering, EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1053.pdf)]
4. Adaptive Document Retrieval for Deep Question Answering, EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1055.pdf)]
5. RankQA: Neural Question Answering with Answer Re-Ranking, ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1611.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/qa.md#rankqa-neural-question-answering-with-answer-re-ranking)]
6. (GRAFT-Net) Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text, EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1455.pdf)]
7. Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader, ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1417.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/qa.md#improving-question-answering-over-incomplete-kbs-with-knowledge-aware-reader)]
8. PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text, EMNLP 2019. [[Paper](https://www.aclweb.org/anthology/D19-1242.pdf)]
9. (MUPPET) Multi-Hop Paragraph Retrieval for Open-Domain Question Answering, ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1222.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/qa.md#muppet-multi-hop-paragraph-retrieval-for-open-domain-question-answering)]
10. (CopyNet) Incorporating Copying Mechanism in Sequence-to-Sequence Learning, ACL 2016. [[Paper](https://www.aclweb.org/anthology/P16-1154.pdf)]
11. Get To The Point: Summarization with Pointer-Generator Networks, ACL 2017. [[Paper](https://www.aclweb.org/anthology/P17-1099.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/qa.md#get-to-the-point-summarization-with-pointer-generator-networks)]
12. Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders, ACL 2018. [[Paper](https://www.aclweb.org/anthology/P18-1204.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/qa.md#learning-to-ask-questions-in-open-domain-conversational-systems-with-typed-decoders)]
13. Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks, EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1424.pdf)]
14. Answer-focused and Position-aware Neural Question Generation, EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1427.pdf)]
15. Interconnected Question Generation with Coreference Alignment and Conversation Flow Modeling, ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1480.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/qa.md#interconnected-question-generation-with-coreference-alignment-and-conversation-flow-modeling)]
16. Reinforced Dynamic Reasoning for Conversational Question Generation, ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1203.pdf)]
17. (QCN) Question Condensing Networks for Answer Selection in Community Question Answering, ACL 2018. [[Paper](https://www.aclweb.org/anthology/P18-1162.pdf)]
18. Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings, EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1452.pdf)]
19. A State-transition Framework to Answer Complex Questions over Knowledge Base, EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1234.pdf)]
20. (SLQA+) Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering, ACL 2018. [[Paper](https://www.aclweb.org/anthology/P18-1158.pdf)]
21. Knowledge-aware Attentive Neural Network for Ranking Question Answer Pairs, SIGIR 2018. [[Paper](https://dl.acm.org/doi/10.1145/3209978.3210081)]
22. Knowledge as A Bridge: Improving Cross-domain Answer Selection with External Knowledge, COLING 2018. [[Paper](https://www.aclweb.org/anthology/C18-1279.pdf)]
23. Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge Base Question Answering, AAAI 2019. [[Paper](https://arxiv.org/pdf/1812.02354.pdf)]
24. Joint Learning of Answer Selection and Answer Summary Generation in Community Question Answering, AAAI 2020. [[Paper](https://arxiv.org/pdf/1911.09801.pdf)]
25. Bridging Hierarchical and Sequential Context Modeling for Question-driven Extractive Answer Summarization, SIGIR 2020. [[Paper](https://dl.acm.org/doi/abs/10.1145/3397271.3401208)]
26. Learning to Ask More: Semi-Autoregressive Sequential Question Generation under Dual-Graph Interaction, ACL 2020. [[Paper](https://www.aclweb.org/anthology/2020.acl-main.21.pdf)] [[Note]()]


## Pretrained Language Models & Transformers
1. (Transformer) Attention Is All You Need, NeurIPS 2017. [[Paper](https://arxiv.org/pdf/1706.03762.pdf)] [[Note](https://github.com/cheneydon/paper_list/blob/main/notes/plm.md#transformer)]
2. (ELMo) Deep Contextualized Word Representations, NAACL 2018. [[Paper](https://arxiv.org/pdf/1802.05365.pdf)] [[Note](https://github.com/cheneydon/paper_list/blob/main/notes/plm.md#2-elmo)]
3. (GPT) Improving Language Understanding by Generative Pre-Training, OpenAI Tech Report 2018. [[Paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)] [[Note](https://github.com/cheneydon/paper_list/blob/main/notes/plm.md#3-gpt)]
4. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, NAACL 2019. [[Paper](https://arxiv.org/pdf/1810.04805.pdf)] [[Note](https://github.com/cheneydon/paper_list/blob/main/notes/plm.md#4-bert)]
5. RoBERTa: A Robustly Optimized BERT Pretraining Approach, arXiv 2019. [[Paper](https://arxiv.org/pdf/1907.11692.pdf)]
6. Context-Aware Representations for Knowledge Base Relation Extraction, EMNLP 2017. [[Paper](https://www.aclweb.org/anthology/D17-1188.pdf)]  
7. Language Models as Knowledge Bases?, EMNLP 2019. [[Paper](https://arxiv.org/pdf/1909.01066.pdf)]
8. Learning to Infer Entities, Properties and their Relations from Clinical Conversations, EMNLP 2019. [[Paper](https://arxiv.org/pdf/1908.11536.pdf)]  
9. AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models, EMNLP 2019. [[Paper](https://arxiv.org/pdf/1909.09251.pdf)]
10. Specializing Word Embeddings (for Parsing) by Information Bottleneck, EMNLP 2019. [[Paper](https://arxiv.org/pdf/1910.00163.pdf)]
11. XLNet: Generalized Autoregressive Pretraining for Language Understanding, NeurIPS 2019. [[Paper](https://arxiv.org/pdf/1906.08237.pdf)]
12. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context, ACL 2019. [[Paper](https://arxiv.org/pdf/1901.02860.pdf)]
13. ERNIE (Tsinghua): Enhanced Language Representation with Informative Entities, ACL 2019. [[Paper](https://arxiv.org/pdf/1905.07129.pdf)]
14. ERNIE (Baidu): Enhanced Representation through Knowledge Integration, arXiv 2019. [[Paper](https://arxiv.org/pdf/1904.09223.pdf)]
15. SpanBERT: Improving Pre-training by Representing and Predicting Spans, arXiv 2019. [[Paper](https://arxiv.org/pdf/1907.10529.pdf)]
16. (MT-DNN) Multi-Task Deep Neural Networks for Natural Language Understanding, ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1441.pdf)]
17. ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding, AAAI 2020. [[Paper](https://arxiv.org/pdf/1907.12412.pdf)]
18. (GPT-2) Language Models are Unsupervised Multitask Learners, OpenAI Tech Report 2019. [[Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)]
19. ELECTRA: Pre-Training Text Encoders as Discriminators Rather Than Generators, ICLR 2020. [[Paper](https://openreview.net/pdf?id=r1xMH1BtvB)]
20. MASS: Masked Sequence to Sequence Pre-training for Language Generation, ICML 2019. [[Paper](https://arxiv.org/pdf/1905.02450.pdf)]
21. (UniLM) Unified Language Model Pre-training for Natural Language Understanding and Generation, NeurIPS 2019. [[Paper](https://arxiv.org/pdf/1905.03197.pdf)]
22. PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization, arXiv 2019. [[Paper](https://arxiv.org/pdf/1912.08777.pdf)]
23. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks, NeurIPS 2019. [[Paper](https://arxiv.org/pdf/1908.02265.pdf)]
24. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, arXiv 2019. [[Paper](https://arxiv.org/pdf/1910.01108.pdf)]  
25. TinyBERT: Distilling BERT for Natural Language Understanding, arXiv 2019. [[Paper](https://arxiv.org/pdf/1909.10351.pdf)]  
26. ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations, ICLR 2020. [[Paper](https://arxiv.org/pdf/1909.11942.pdf)]
27. (BERT-PKD) Patient Knowledge Distillation for BERT Model Compression, EMNLP 2019. [[Paper](https://arxiv.org/pdf/1908.09355.pdf)]
28. Lite Transformer with Long-Short Range Attention, ICLR 2020. [[Paper](https://openreview.net/pdf?id=ByeMPlHKPH)]
29. Pay Less Attention with Lightweight and Dynamic Convolutions, ICLR 2019. [[Paper](https://openreview.net/pdf?id=SkVhlh09tX)]
30. BERT-of-Theseus: Compressing BERT by Progressive Module Replacing, arXiv 2020. [[Paper](https://arxiv.org/pdf/2002.02925.pdf)]
31. MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices, ACL 2020. [[Paper](https://arxiv.org/pdf/2004.02984.pdf)]
32. StructBERT: Incorporating Language Structures into Pretraining for Deep language Understanding, ICLR 2020. [[Paper](https://openreview.net/pdf?id=BJgQ4lSFPH)]
33. (LAMBADA) Do Not Have Enough Data? Deep Learning to the Rescue!, AAAI 2020. [[Paper](https://arxiv.org/pdf/1911.03118.pdf)]
34. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension, arXiv 2020. [[Paper](https://arxiv.org/pdf/1910.13461.pdf)]
35. Towards Making the Most of BERT in Neural Machine Translation, arXiv 2019. [[Paper](https://arxiv.org/pdf/1908.05672.pdf)]
36. MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning, arXiv 2019. [[paper](https://arxiv.org/pdf/1911.09483.pdf)]
37. K-BERT: Enabling Language Representation with Knowledge Graph, AAAI 2020. [[Paper](https://arxiv.org/pdf/1909.07606.pdf)]
38. Universal Transformers, ICLR 2019. [[Paper](https://openreview.net/pdf?id=HyzdRiR9Y7)]
39. Depth-Adaptive Transformer, ICLR 2020. [[Paper](https://openreview.net/pdf?id=SJg7KhVKPH)]
40. FastBERT: a Self-distilling BERT with Adaptive Inference Time, ACL 2020. [[Paper](https://arxiv.org/pdf/2004.02178.pdf)]
41. DynaBERT: Dynamic BERT with Adaptive Width and Depth, arXiv 2020. [[Paper](https://arxiv.org/pdf/2004.04037.pdf)]
42. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers, arXiv 2020. [[Paper](https://arxiv.org/pdf/2002.10957.pdf)]
43. DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering, ACL 2020. [[Paper](https://arxiv.org/pdf/2005.00697.pdf)]
44. Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing, arXiv 2020. [[Paper](https://arxiv.org/pdf/2006.03236.pdf)]
45. Linformer: Self-Attention with Linear Complexity, arXiv 2020. [[Paper](https://arxiv.org/pdf/2006.04768.pdf)]
46. SqueezeBERT: What can computer vision teach NLP about efficient neural networks?, arXiv 2020. [[Paper](https://arxiv.org/pdf/2006.11316.pdf)]
47. Synthesizer: Rethinking Self-Attention in Transformer Models, arXiv 2020. [[Paper](https://arxiv.org/pdf/2005.00743.pdf)]
48. Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers, ICML 2020. [[Paper](https://arxiv.org/pdf/2002.11794.pdf)]
49. Don't Stop Pretraining: Adapt Language Models to Domains and Tasks, ACL 2020. [[Paper](https://arxiv.org/pdf/2004.10964.pdf)]
50. DeLighT: Very Deep and Light-weight Transformer, arXiv 2020. [[Paper](https://arxiv.org/pdf/2008.00623.pdf)]
51. Generating Long Sequences with Sparse Transformers, arXiv 2019. [[Paper](https://arxiv.org/pdf/1904.10509.pdf)]
52. Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection, arXiv 2019. [[Paper](https://arxiv.org/pdf/1912.11637.pdf)]
53. Longformer: The Long-Document Transformer, arXiv 2020. [[Paper](https://arxiv.org/pdf/2004.05150.pdf)]
54. Big Bird: Transformers for Longer Sequences, arXiv 2020. [[Paper](https://arxiv.org/pdf/2007.14062.pdf)]
55. Very Deep Transformers for Neural Machine Translation, arXiv 2020. [[Paper](https://arxiv.org/pdf/2008.07772.pdf)]
56. AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization, arXiv 2020. [[Paper](https://arxiv.org/pdf/2008.11869.pdf)]
57. Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation, arXiv 2020. [[Paper](https://arxiv.org/pdf/2002.10345.pdf)]


## AutoML & Dynamic Networks & Self-Supervised Learning
1. AutoML: A Survey of the State-of-the-Art, arXiv 2019. [[Paper](https://arxiv.org/pdf/1908.00709.pdf)]
2. (ENAS) Efficient Neural Architecture Search via Parameter Sharing, ICML 2018. [[Paper](https://arxiv.org/pdf/1802.03268.pdf)] [[Note](https://github.com/cheneydon/paper_list/blob/main/notes/automl.md#enas)]
3. (NASNet) Learning Transferable Architectures for Scalable Image Recognition, CVPR 2018. [[Paper](https://arxiv.org/pdf/1707.07012.pdf)]
4. DARTS: Differentiable Architecture Search, ICLR 2019. [[Paper](https://arxiv.org/pdf/1806.09055.pdf)] [[Note](https://github.com/cheneydon/paper_list/blob/main/notes/automl.md#darts)]
5. FBNet: Hardware-Aware Efficient ConvNet Design, CVPR 2019. [[Paper](https://arxiv.org/pdf/1812.03443.pdf)] [[Note](https://github.com/cheneydon/paper_list/blob/main/notes/automl.md#fbnet)]
6. SNAS: Stochastic Neural Architecture Search, ICLR 2019. [[Paper](https://arxiv.org/pdf/1812.09926.pdf)]
7. (GeNet) Genetic CNN, ICCV 2017. [[Paper](https://arxiv.org/pdf/1703.01513.pdf)] [[Note](https://github.com/cheneydon/paper_list/blob/main/notes/automl.md#genet)]
8. (AmoebaNet) Regularized Evolution for Image Classifier Architecture Search, AAAI 2019. [[Paper](https://arxiv.org/pdf/1802.01548.pdf)]
9. (PNAS) Progressive Neural Architecture Search, ECCV 2018. [[Paper](https://arxiv.org/pdf/1712.00559.pdf)] [[Note](https://github.com/cheneydon/paper_list/blob/main/notes/automl.md#pnas)]
10. ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware, ICLR 2019. [[Paper](https://arxiv.org/pdf/1812.00332.pdf)]
11. (One-Shot) Understanding and Simplifying One-Shot Architecture Search, ICML 2018. [[Paper](http://proceedings.mlr.press/v80/bender18a/bender18a.pdf)]
12. (SPOS) Single Path One-Shot Neural Architecture Search with Uniform Sampling, arXiv 2019. [[Paper](https://arXiv.org/pdf/1904.00420.pdf)] [[Note](https://github.com/cheneydon/paper_list/blob/main/notes/automl.md#spos)]
13. (NAO) Neural Architecture Optimization, NeurIPS 2018. [[Paper](https://arxiv.org/pdf/1808.07233.pdf)] [[Note](https://github.com/cheneydon/paper_list/blob/main/notes/automl.md#nao)]
14. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, ICML 2019. [[Paper](https://arxiv.org/pdf/1905.11946.pdf)]
15. (PC-NAS) Improving One-Shot NAS by Suppressing the Posterior Fading, arXiv 2019. [[Paper](https://openreview.net/pdf?id=HJgJNCEKPr)]
16. FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search, arXiv 2019. [[Paper](https://arxiv.org/pdf/1907.01845.pdf)]
17. (LaNAS) Sample-Efficient Neural Architecture Search by Learning Action Space, arXiv 2019. [[Paper](https://arxiv.org/pdf/1906.06832.pdf)]
18. The Evolved Transformer, ICML 2019. [[Paper](https://arxiv.org/pdf/1901.11117.pdf)]
19. Language Models with Transformers, arXiv 2019. [[Paper](https://arxiv.org/pdf/1904.09408.pdf)]
20. Continual and Multi-Task Architecture Search, ACL 2019. [[Paper](https://arxiv.org/pdf/1906.05226.pdf)]
21. AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture, arXiv 2019. [[Paper](https://arxiv.org/pdf/1911.09251.pdf)]
22. WeNet: Weighted Networks for Recurrent Network Architecture Search, arXiv 2019. [[Paper](https://arxiv.org/pdf/1904.03819.pdf)]
23. Improved Differentiable Architecture Search for Language Modeling and Named Entity Recognition, EMNLP 2019. [[Paper](https://www.aclweb.org/anthology/D19-1367.pdf)]
24. (DNA) Blockwisely Supervised Neural Architecture Search with Knowledge Distillation, CVPR 2020. [[Paper](https://arxiv.org/pdf/1911.13053.pdf)]
25. AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search, arXiv 2020. [[Paper](https://arxiv.org/pdf/2001.04246.pdf)]
26. AutoML-Zero: Evolving Machine Learning Algorithms From Scratch, arXiv 2020. [[Paper](https://arxiv.org/pdf/2003.03384.pdf)]
27. Once-for-All: Train One Network and Specialize it for Efficient Deployment on Diverse Hardware Platforms, ICLR 2020. [[Paper](https://arxiv.org/pdf/1908.09791.pdf)]
28. Are Labels Necessary for Neural Architecture Search?, arXiv 2020. [[Paper](https://arxiv.org/pdf/2003.12056.pdf)]
29. BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models, arXiv 2020. [[Paper](https://arxiv.org/pdf/2003.11142.pdf)]
30. Neural Architecture Search for Lightweight Non-Local Networks, CVPR 2020. [[Paper](https://arxiv.org/pdf/2004.01961.pdf)]
31. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing, ACL 2020. [[Paper](https://arxiv.org/pdf/2005.14187.pdf)]
32. FBNetV2: Differentiable Neural Architecture Search for Spatial and Channel Dimensions, CVPR 2020. [[Paper](https://arxiv.org/pdf/2004.05565.pdf)]
33. FBNetV3: Joint Architecture-Recipe Search using Neural Acquisition Function, arXiv 2020. [[Paper](https://arxiv.org/pdf/2006.02049.pdf)]
34. Evolving Normalization-Activation Layers, arXiv 2020. [[Paper](https://arxiv.org/pdf/2004.02967.pdf)]
35. Computation Reallocation for Object Detection, ICLR 2020. [[Paper](https://arxiv.org/pdf/1912.11234.pdf)]
36. Learning Dynamic Routing for Semantic Segmentation, CVPR 2020. [[Paper](https://arxiv.org/pdf/2003.10401.pdf)]
37. Slimmable Neural Networks, ICLR 2019. [[Paper](https://openreview.net/pdf?id=H1gMCsAqY7)]
38. Universally Slimmable Networks and Improved Training Techniques, ICCV 2019. [[Paper](https://arxiv.org/pdf/1903.05134.pdf)]
39. AutoSlim: Towards One-Shot Architecture Search for Channel Numbers, arXiv 2019. [[Paper](https://arxiv.org/pdf/1903.11728.pdf)]
40. Resolution Adaptive Networks for Efficient Inference, CVPR 2020. [[Paper](https://arxiv.org/pdf/2003.07326.pdf)]
41. (MoCo) Momentum Contrast for Unsupervised Visual Representation Learning, CVPR 2020. [[Paper](https://arxiv.org/pdf/1911.05722.pdf)]
42. (SimCLR) A Simple Framework for Contrastive Learning of Visual Representations, arXiv 2020. [[Paper](https://arxiv.org/pdf/2002.05709.pdf)]
43. (MoCo v2) Improved Baselines with Momentum Contrastive Learning, Facebook Tech Report 2020. [[Paper](https://arxiv.org/pdf/2003.04297.pdf)]
44. (SimCLR v2) Big Self-Supervised Models are Strong Semi-Supervised Learners, arXiv 2020. [[Paper](https://arxiv.org/pdf/2006.10029.pdf)]
45. (BYOL) Bootstrap Your Own Latent A New Approach to Self-Supervised Learning, arXiv 2020. [[Paper](https://arxiv.org/pdf/2006.07733.pdf)]
46. AutoTrans: Automating Transformer Design via Reinforced Architecture, arXiv 2020. [[Paper](https://arxiv.org/pdf/2009.02070.pdf)]
47. Tiny Transfer Learning: Towards Memory-Efficient On-Device Learning, arXiv 2020. [[Paper](https://arxiv.org/pdf/2007.11622.pdf)]
