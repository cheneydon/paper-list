# Paper List
Record the papers I have read so far.

## Pretrained Language Models & Transformers
1. (Transformer) Attention Is All You Need. NeurIPS 2017. [[Paper](https://arxiv.org/pdf/1706.03762.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/plm.md#transformer)]
2. (ELMo) Deep Contextualized Word Representations. NAACL 2018. [[Paper](https://arxiv.org/pdf/1802.05365.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/plm.md#2-elmo)]
3. (GPT) Improving Language Understanding by Generative Pre-Training. OpenAI Tech Report 2018. [[Paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/plm.md#3-gpt)]
4. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL 2019. [[Paper](https://arxiv.org/pdf/1810.04805.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/plm.md#4-bert)]
5. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv 2019. [[Paper](https://arxiv.org/pdf/1907.11692.pdf)]
6. Context-Aware Representations for Knowledge Base Relation Extraction. EMNLP 2017. [[Paper](https://www.aclweb.org/anthology/D17-1188.pdf)]  
7. Language Models as Knowledge Bases?. EMNLP 2019. [[Paper](https://arxiv.org/pdf/1909.01066.pdf)]
8. Learning to Infer Entities, Properties and their Relations from Clinical Conversations. EMNLP 2019. [[Paper](https://arxiv.org/pdf/1908.11536.pdf)]  
9. AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models. EMNLP 2019. [[Paper](https://arxiv.org/pdf/1909.09251.pdf)]
10. Specializing Word Embeddings (for Parsing) by Information Bottleneck. EMNLP 2019. [[Paper](https://arxiv.org/pdf/1910.00163.pdf)]
11. XLNet: Generalized Autoregressive Pretraining for Language Understanding. NeurIPS 2019. [[Paper](https://arxiv.org/pdf/1906.08237.pdf)]
12. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. ACL 2019. [[Paper](https://arxiv.org/pdf/1901.02860.pdf)]
13. ERNIE (Tsinghua): Enhanced Language Representation with Informative Entities. ACL 2019. [[Paper](https://arxiv.org/pdf/1905.07129.pdf)]
14. ERNIE (Baidu): Enhanced Representation through Knowledge Integration. arXiv 2019. [[Paper](https://arxiv.org/pdf/1904.09223.pdf)]
15. SpanBERT: Improving Pre-training by Representing and Predicting Spans. TACL 2020. [[Paper](https://arxiv.org/pdf/1907.10529.pdf)]
16. (MT-DNN) Multi-Task Deep Neural Networks for Natural Language Understanding. ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1441.pdf)]
17. ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding. AAAI 2020. [[Paper](https://arxiv.org/pdf/1907.12412.pdf)]
18. (GPT-2) Language Models are Unsupervised Multitask Learners. OpenAI Tech Report 2019. [[Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)]
19. ELECTRA: Pre-Training Text Encoders as Discriminators Rather Than Generators. ICLR 2020. [[Paper](https://openreview.net/pdf?id=r1xMH1BtvB)]
20. MASS: Masked Sequence to Sequence Pre-training for Language Generation. ICML 2019. [[Paper](https://arxiv.org/pdf/1905.02450.pdf)]
21. (UniLM) Unified Language Model Pre-training for Natural Language Understanding and Generation. NeurIPS 2019. [[Paper](https://arxiv.org/pdf/1905.03197.pdf)]
22. PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. ICML 2020. [[Paper](https://arxiv.org/pdf/1912.08777.pdf)]
23. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. NeurIPS 2019. [[Paper](https://arxiv.org/pdf/1908.02265.pdf)]
24. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. NeurIPS Workshop 2019. [[Paper](https://arxiv.org/pdf/1910.01108.pdf)]  
25. TinyBERT: Distilling BERT for Natural Language Understanding. EMNLP 2020. [[Paper](https://arxiv.org/pdf/1909.10351.pdf)]  
26. ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations. ICLR 2020. [[Paper](https://arxiv.org/pdf/1909.11942.pdf)]
27. (BERT-PKD) Patient Knowledge Distillation for BERT Model Compression. EMNLP 2019. [[Paper](https://arxiv.org/pdf/1908.09355.pdf)]
28. Lite Transformer with Long-Short Range Attention. ICLR 2020. [[Paper](https://openreview.net/pdf?id=ByeMPlHKPH)]
29. Pay Less Attention with Lightweight and Dynamic Convolutions. ICLR 2019. [[Paper](https://openreview.net/pdf?id=SkVhlh09tX)]
30. BERT-of-Theseus: Compressing BERT by Progressive Module Replacing. EMNLP 2020. [[Paper](https://arxiv.org/pdf/2002.02925.pdf)]
31. MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices. ACL 2020. [[Paper](https://arxiv.org/pdf/2004.02984.pdf)]
32. StructBERT: Incorporating Language Structures into Pretraining for Deep language Understanding. ICLR 2020. [[Paper](https://openreview.net/pdf?id=BJgQ4lSFPH)]
33. (LAMBADA) Do Not Have Enough Data? Deep Learning to the Rescue!. AAAI 2020. [[Paper](https://arxiv.org/pdf/1911.03118.pdf)]
34. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. ACL 2020. [[Paper](https://arxiv.org/pdf/1910.13461.pdf)]
35. Towards Making the Most of BERT in Neural Machine Translation. AAAI 2020. [[Paper](https://arxiv.org/pdf/1908.05672.pdf)]
36. MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning. arXiv 2019. [[paper](https://arxiv.org/pdf/1911.09483.pdf)]
37. K-BERT: Enabling Language Representation with Knowledge Graph. AAAI 2020. [[Paper](https://arxiv.org/pdf/1909.07606.pdf)]
38. Universal Transformers. ICLR 2019. [[Paper](https://openreview.net/pdf?id=HyzdRiR9Y7)]
39. Depth-Adaptive Transformer. ICLR 2020. [[Paper](https://openreview.net/pdf?id=SJg7KhVKPH)]
40. FastBERT: a Self-distilling BERT with Adaptive Inference Time. ACL 2020. [[Paper](https://arxiv.org/pdf/2004.02178.pdf)]
41. DynaBERT: Dynamic BERT with Adaptive Width and Depth. NeurIPS 2020. [[Paper](https://arxiv.org/pdf/2004.04037.pdf)]
42. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. NeurIPS 2020. [[Paper](https://arxiv.org/pdf/2002.10957.pdf)]
43. DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering. ACL 2020. [[Paper](https://arxiv.org/pdf/2005.00697.pdf)]
44. Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing. arXiv 2020. [[Paper](https://arxiv.org/pdf/2006.03236.pdf)]
45. Linformer: Self-Attention with Linear Complexity. arXiv 2020. [[Paper](https://arxiv.org/pdf/2006.04768.pdf)]
46. SqueezeBERT: What can computer vision teach NLP about efficient neural networks?. arXiv 2020. [[Paper](https://arxiv.org/pdf/2006.11316.pdf)]
47. Synthesizer: Rethinking Self-Attention in Transformer Models. arXiv 2020. [[Paper](https://arxiv.org/pdf/2005.00743.pdf)]
48. Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers. ICML 2020. [[Paper](https://arxiv.org/pdf/2002.11794.pdf)]
49. Don't Stop Pretraining: Adapt Language Models to Domains and Tasks. ACL 2020. [[Paper](https://arxiv.org/pdf/2004.10964.pdf)]
50. DeLighT: Very Deep and Light-weight Transformer. arXiv 2020. [[Paper](https://arxiv.org/pdf/2008.00623.pdf)]
51. Generating Long Sequences with Sparse Transformers. arXiv 2019. [[Paper](https://arxiv.org/pdf/1904.10509.pdf)]
52. Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection. arXiv 2019. [[Paper](https://arxiv.org/pdf/1912.11637.pdf)]
53. Longformer: The Long-Document Transformer. arXiv 2020. [[Paper](https://arxiv.org/pdf/2004.05150.pdf)]
54. Big Bird: Transformers for Longer Sequences. NeurIPS 2020. [[Paper](https://arxiv.org/pdf/2007.14062.pdf)]
55. Very Deep Transformers for Neural Machine Translation. arXiv 2020. [[Paper](https://arxiv.org/pdf/2008.07772.pdf)]
56. AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization. arXiv 2020. [[Paper](https://arxiv.org/pdf/2008.11869.pdf)]
57. Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation. arXiv 2020. [[Paper](https://arxiv.org/pdf/2002.10345.pdf)]
58. (Bort) Optimal Subarchitecture Extraction For BERT. arXiv 2020. [[Paper](https://arxiv.org/pdf/2010.10499.pdf)]
59. ConvBERT: Improving BERT with Span-based Dynamic Convolution. NeurIPS 2020. [[Paper](https://arxiv.org/pdf/2008.02496.pdf)]
60. Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation. AAAI 2021. [[Paper](https://www.researchgate.net/publication/348647851_Learning_to_Augment_for_Data-Scarce_Domain_BERT_Knowledge_Distillation)]
61. BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Moverâ€™s Distance. EMNLP 2020. [[Paper](https://arxiv.org/pdf/2010.06133.pdf)]
62. LRC-BERT: Latent-representation Contrastive Knowledge Distillation for Natural Language Understanding. AAAI 2021. [[Paper](https://arxiv.org/pdf/2012.07335.pdf)]
63. EdgeBERT: Optimizing On-Chip Inference for Multi-Task NLP. arXiv 2020. [[Paper](https://arxiv.org/pdf/2011.14203.pdf)]


## AutoML & Dynamic Networks
1. AutoML: A Survey of the State-of-the-Art. arXiv 2019. [[Paper](https://arxiv.org/pdf/1908.00709.pdf)]
2. (ENAS) Efficient Neural Architecture Search via Parameter Sharing. ICML 2018. [[Paper](https://arxiv.org/pdf/1802.03268.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/automl.md#enas)]
3. (NASNet) Learning Transferable Architectures for Scalable Image Recognition. CVPR 2018. [[Paper](https://arxiv.org/pdf/1707.07012.pdf)]
4. DARTS: Differentiable Architecture Search. ICLR 2019. [[Paper](https://arxiv.org/pdf/1806.09055.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/automl.md#darts)]
5. FBNet: Hardware-Aware Efficient ConvNet Design. CVPR 2019. [[Paper](https://arxiv.org/pdf/1812.03443.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/automl.md#fbnet)]
6. SNAS: Stochastic Neural Architecture Search. ICLR 2019. [[Paper](https://arxiv.org/pdf/1812.09926.pdf)]
7. (GeNet) Genetic CNN. ICCV 2017. [[Paper](https://arxiv.org/pdf/1703.01513.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/automl.md#genet)]
8. (AmoebaNet) Regularized Evolution for Image Classifier Architecture Search. AAAI 2019. [[Paper](https://arxiv.org/pdf/1802.01548.pdf)]
9. (PNAS) Progressive Neural Architecture Search. ECCV 2018. [[Paper](https://arxiv.org/pdf/1712.00559.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/automl.md#pnas)]
10. ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware. ICLR 2019. [[Paper](https://arxiv.org/pdf/1812.00332.pdf)]
11. (One-Shot) Understanding and Simplifying One-Shot Architecture Search. ICML 2018. [[Paper](http://proceedings.mlr.press/v80/bender18a/bender18a.pdf)]
12. (SPOS) Single Path One-Shot Neural Architecture Search with Uniform Sampling. ECCV 2020. [[Paper](https://arXiv.org/pdf/1904.00420.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/automl.md#spos)]
13. (NAO) Neural Architecture Optimization. NeurIPS 2018. [[Paper](https://arxiv.org/pdf/1808.07233.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/en/automl.md#nao)]
14. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. ICML 2019. [[Paper](https://arxiv.org/pdf/1905.11946.pdf)]
15. (PC-NAS) Improving One-Shot NAS by Suppressing the Posterior Fading. CVPR 2020. [[Paper](https://openreview.net/pdf?id=HJgJNCEKPr)]
16. FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search. arXiv 2019. [[Paper](https://arxiv.org/pdf/1907.01845.pdf)]
17. (LaNAS) Sample-Efficient Neural Architecture Search by Learning Action Space. arXiv 2019. [[Paper](https://arxiv.org/pdf/1906.06832.pdf)]
18. The Evolved Transformer. ICML 2019. [[Paper](https://arxiv.org/pdf/1901.11117.pdf)]
19. Language Models with Transformers. arXiv 2019. [[Paper](https://arxiv.org/pdf/1904.09408.pdf)]
20. Continual and Multi-Task Architecture Search. ACL 2019. [[Paper](https://arxiv.org/pdf/1906.05226.pdf)]
21. AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture. AAAI 2020. [[Paper](https://arxiv.org/pdf/1911.09251.pdf)]
22. WeNet: Weighted Networks for Recurrent Network Architecture Search. arXiv 2019. [[Paper](https://arxiv.org/pdf/1904.03819.pdf)]
23. Improved Differentiable Architecture Search for Language Modeling and Named Entity Recognition. EMNLP 2019. [[Paper](https://www.aclweb.org/anthology/D19-1367.pdf)]
24. (DNA) Blockwisely Supervised Neural Architecture Search with Knowledge Distillation. CVPR 2020. [[Paper](https://arxiv.org/pdf/1911.13053.pdf)]
25. AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search. IJCAI 2020. [[Paper](https://arxiv.org/pdf/2001.04246.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/nas.md#adabert-task-adaptive-bert-compression-with-differentiable-neural-architecture-search)]
26. AutoML-Zero: Evolving Machine Learning Algorithms From Scratch. ICML 2020. [[Paper](https://arxiv.org/pdf/2003.03384.pdf)]
27. Once-for-All: Train One Network and Specialize it for Efficient Deployment on Diverse Hardware Platforms. ICLR 2020. [[Paper](https://arxiv.org/pdf/1908.09791.pdf)]
28. Are Labels Necessary for Neural Architecture Search?. ECCV 2020. [[Paper](https://arxiv.org/pdf/2003.12056.pdf)]
29. BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models. ECCV 2020. [[Paper](https://arxiv.org/pdf/2003.11142.pdf)]
30. Neural Architecture Search for Lightweight Non-Local Networks. CVPR 2020. [[Paper](https://arxiv.org/pdf/2004.01961.pdf)]
31. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing. ACL 2020. [[Paper](https://arxiv.org/pdf/2005.14187.pdf)]
32. FBNetV2: Differentiable Neural Architecture Search for Spatial and Channel Dimensions. CVPR 2020. [[Paper](https://arxiv.org/pdf/2004.05565.pdf)]
33. FBNetV3: Joint Architecture-Recipe Search using Neural Acquisition Function. arXiv 2020. [[Paper](https://arxiv.org/pdf/2006.02049.pdf)]
34. Evolving Normalization-Activation Layers. arXiv 2020. [[Paper](https://arxiv.org/pdf/2004.02967.pdf)]
35. Computation Reallocation for Object Detection. ICLR 2020. [[Paper](https://arxiv.org/pdf/1912.11234.pdf)]
36. Learning Dynamic Routing for Semantic Segmentation. CVPR 2020. [[Paper](https://arxiv.org/pdf/2003.10401.pdf)]
37. Slimmable Neural Networks. ICLR 2019. [[Paper](https://openreview.net/pdf?id=H1gMCsAqY7)]
38. Universally Slimmable Networks and Improved Training Techniques. ICCV 2019. [[Paper](https://arxiv.org/pdf/1903.05134.pdf)]
39. AutoSlim: Towards One-Shot Architecture Search for Channel Numbers. arXiv 2019. [[Paper](https://arxiv.org/pdf/1903.11728.pdf)]
40. Resolution Adaptive Networks for Efficient Inference. CVPR 2020. [[Paper](https://arxiv.org/pdf/2003.07326.pdf)]
41. AutoTrans: Automating Transformer Design via Reinforced Architecture. arXiv 2020. [[Paper](https://arxiv.org/pdf/2009.02070.pdf)]
42. Tiny Transfer Learning: Towards Memory-Efficient On-Device Learning. arXiv 2020. [[Paper](https://arxiv.org/pdf/2007.11622.pdf)]
43. Task-Agnostic and Adaptive-Size BERT Compression. arXiv 2020. [[Paper](https://openreview.net/pdf?id=wZ4yWvQ_g2y)]
44. Interactive Feature Generation via Learning Adjacency Tensor of Feature Graph. arXiv 2020. [[Paper](https://arxiv.org/pdf/2007.14573.pdf)]
45. Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search. ICLR 2021. [[Paper](https://openreview.net/pdf?id=5jzlpHvvRk)]


## Question Answering
1. (DrQA) Reading Wikipedia to Answer Open-Domain Questions. ACL 2017. [[Paper](https://www.aclweb.org/anthology/P17-1171.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#drqa-reading-wikipedia-to-answer-open-domain-questions)]
2. Denoising Distantly Supervised Open-Domain Question Answering. ACL 2018. [[Paper](https://www.aclweb.org/anthology/P18-1161.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#denoising-distantly-supervised-open-domain-question-answering)]
3. Ranking Paragraphs for Improving Answer Recall in Open-Domain Question Answering. EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1053.pdf)]
4. Adaptive Document Retrieval for Deep Question Answering. EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1055.pdf)]
5. RankQA: Neural Question Answering with Answer Re-Ranking. ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1611.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#rankqa-neural-question-answering-with-answer-re-ranking)]
6. (GRAFT-Net) Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text. EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1455.pdf)]
7. Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader. ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1417.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#improving-question-answering-over-incomplete-kbs-with-knowledge-aware-reader)]
8. PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text. EMNLP 2019. [[Paper](https://www.aclweb.org/anthology/D19-1242.pdf)]
9. (MUPPET) Multi-Hop Paragraph Retrieval for Open-Domain Question Answering. ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1222.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#muppet-multi-hop-paragraph-retrieval-for-open-domain-question-answering)]
10. (CopyNet) Incorporating Copying Mechanism in Sequence-to-Sequence Learning. ACL 2016. [[Paper](https://www.aclweb.org/anthology/P16-1154.pdf)]
11. Get To The Point: Summarization with Pointer-Generator Networks. ACL 2017. [[Paper](https://www.aclweb.org/anthology/P17-1099.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#get-to-the-point-summarization-with-pointer-generator-networks)]
12. Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders. ACL 2018. [[Paper](https://www.aclweb.org/anthology/P18-1204.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#learning-to-ask-questions-in-open-domain-conversational-systems-with-typed-decoders)]
13. Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks. EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1424.pdf)]
14. Answer-focused and Position-aware Neural Question Generation. EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1427.pdf)]
15. Interconnected Question Generation with Coreference Alignment and Conversation Flow Modeling. ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1480.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#interconnected-question-generation-with-coreference-alignment-and-conversation-flow-modeling)]
16. Reinforced Dynamic Reasoning for Conversational Question Generation. ACL 2019. [[Paper](https://www.aclweb.org/anthology/P19-1203.pdf)]
17. (QCN) Question Condensing Networks for Answer Selection in Community Question Answering. ACL 2018. [[Paper](https://www.aclweb.org/anthology/P18-1162.pdf)]
18. Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings. EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1452.pdf)]
19. A State-transition Framework to Answer Complex Questions over Knowledge Base. EMNLP 2018. [[Paper](https://www.aclweb.org/anthology/D18-1234.pdf)]
20. (SLQA+) Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering. ACL 2018. [[Paper](https://www.aclweb.org/anthology/P18-1158.pdf)]
21. Knowledge-aware Attentive Neural Network for Ranking Question Answer Pairs. SIGIR 2018. [[Paper](https://dl.acm.org/doi/10.1145/3209978.3210081)]
22. Knowledge as A Bridge: Improving Cross-domain Answer Selection with External Knowledge. COLING 2018. [[Paper](https://www.aclweb.org/anthology/C18-1279.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#knowledge-as-a-bridge-improving-cross-domain-answer-selection-with-external-knowledge)]
23. Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge Base Question Answering. AAAI 2019. [[Paper](https://arxiv.org/pdf/1812.02354.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#multi-task-learning-with-multi-view-attention-for-answer-selection-and-knowledge-base-question-answering)]
24. Joint Learning of Answer Selection and Answer Summary Generation in Community Question Answering. AAAI 2020. [[Paper](https://arxiv.org/pdf/1911.09801.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#joint-learning-of-answer-selection-and-answer-summary-generation-in-community-question-answering)]
25. Bridging Hierarchical and Sequential Context Modeling for Question-driven Extractive Answer Summarization. SIGIR 2020. [[Paper](https://dl.acm.org/doi/abs/10.1145/3397271.3401208)]
26. Learning to Ask More: Semi-Autoregressive Sequential Question Generation under Dual-Graph Interaction. ACL 2020. [[Paper](https://www.aclweb.org/anthology/2020.acl-main.21.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#learning-to-ask-more-semi-autoregressive-sequential-question-generation-under-dual-graph-interaction)]
27. How to Ask Good Questions? Try to Leverage Paraphrases. ACL 2020. [[Paper](https://www.aclweb.org/anthology/2020.acl-main.545.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/qa.md#how-to-ask-good-questions-try-to-leverage-paraphrases)]
28. Knowledge Enhanced Latent Relevance Mining for Question Answering. ICASSP 2020. [[Paper](https://ieeexplore.ieee.org/document/9053432)]


## Meta-Learning
1. Automated Relational Meta-learning. ICLR 2020. [[Paper](https://arxiv.org/pdf/2001.00745.pdf)] [[Note](https://github.com/cheneydon/paper-list/blob/main/notes/cn/meta_learning.md#automated-relational-meta-learning)]
2. Multi-source Meta Transfer for Low Resource Multiple-Choice Question Answering. ACL 2020. [[Paper](https://www.aclweb.org/anthology/2020.acl-main.654.pdf)]
3. Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining. EMNLP 2020. [[Paper](https://arxiv.org/pdf/2003.13003.pdf)]
4. Pre-training Text Representations as Meta Learning. arXiv 2020. [[Paper](https://arxiv.org/pdf/2004.05568.pdf)]
5. Hierarchically Structured Meta-learning. ICML 2019. [[Paper](https://arxiv.org/pdf/1905.05301.pdf)]
6. Online Structured Meta-learning. NeurIPS 2020. [[Paper](https://arxiv.org/pdf/2010.11545.pdf)]


##  Contrastive Learning
1. (MoCo) Momentum Contrast for Unsupervised Visual Representation Learning. CVPR 2020. [[Paper](https://arxiv.org/pdf/1911.05722.pdf)]
2. (SimCLR) A Simple Framework for Contrastive Learning of Visual Representations. ICML 2020. [[Paper](https://arxiv.org/pdf/2002.05709.pdf)]
3. (MoCo v2) Improved Baselines with Momentum Contrastive Learning. Facebook Tech Report 2020. [[Paper](https://arxiv.org/pdf/2003.04297.pdf)]
4. (SimCLR v2) Big Self-Supervised Models are Strong Semi-Supervised Learners. NeurIPS 2020. [[Paper](https://arxiv.org/pdf/2006.10029.pdf)]
5. (BYOL) Bootstrap Your Own Latent A New Approach to Self-Supervised Learning. arXiv 2020. [[Paper](https://arxiv.org/pdf/2006.07733.pdf)]
6. Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning. arXiv 2020. [[Paper](https://arxiv.org/pdf/2011.01403.pdf)]
